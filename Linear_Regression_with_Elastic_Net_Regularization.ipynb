{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zc02sM5D6aG1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class ElasticNetLinearRegression:\n",
        "    def __init__(self, alpha=1.0, l1_ratio=0.5, learning_rate=0.01, iterations=1000):\n",
        "        \"\"\"\n",
        "        Elastic Net Linear Regression constructor.\n",
        "\n",
        "        Parameters:\n",
        "        alpha : Regularization strength (alpha > 0). Higher alpha increases regularization.\n",
        "        l1_ratio : The ratio between L1 (Lasso) and L2 (Ridge) regularization.\n",
        "                   l1_ratio = 0 corresponds to L2 penalty (Ridge),\n",
        "                   l1_ratio = 1 corresponds to L1 penalty (Lasso),\n",
        "                   and 0 < l1_ratio < 1 corresponds to Elastic Net.\n",
        "        learning_rate : The step size for gradient descent. Controls how much to adjust the model per iteration.\n",
        "        iterations : Number of iterations for the gradient descent optimization.\n",
        "        \"\"\"\n",
        "        self.alpha = alpha  # Store regularization strength\n",
        "        self.l1_ratio = l1_ratio  # Store ratio between L1 and L2 regularization\n",
        "        self.learning_rate = learning_rate  # Store learning rate for gradient descent\n",
        "        self.iterations = iterations  # Store number of iterations for gradient descent\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model using gradient descent to minimize the cost function.\n",
        "\n",
        "        Parameters:\n",
        "        X : Input feature matrix (num_samples x num_features)\n",
        "        y : Target vector (num_samples,)\n",
        "        \"\"\"\n",
        "        self.m, self.n = X.shape  # Number of samples (m) and number of features (n)\n",
        "        self.theta = np.zeros(self.n)  # Initialize weights (coefficients) to zeros\n",
        "        self.bias = 0  # Initialize bias (intercept) to zero\n",
        "\n",
        "        # Gradient descent optimization loop\n",
        "        for _ in range(self.iterations):\n",
        "            y_pred = self.predict(X)  # Compute predictions based on current weights and bias\n",
        "\n",
        "            # Compute gradients for weights (theta) using Elastic Net regularization\n",
        "            d_theta = (1 / self.m) * (X.T @ (y_pred - y)) + self.alpha * (\n",
        "                self.l1_ratio * np.sign(self.theta) +  # L1 penalty (Lasso)\n",
        "                (1 - self.l1_ratio) * self.theta)  # L2 penalty (Ridge)\n",
        "\n",
        "            # Compute gradient for bias (intercept)\n",
        "            d_bias = (1 / self.m) * np.sum(y_pred - y)\n",
        "\n",
        "            # Update weights and bias using the gradients\n",
        "            self.theta -= self.learning_rate * d_theta  # Update weights\n",
        "            self.bias -= self.learning_rate * d_bias  # Update bias\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions using the learned weights and bias.\n",
        "\n",
        "        Parameters:\n",
        "        X : Input feature matrix (num_samples x num_features)\n",
        "\n",
        "        Returns:\n",
        "        y_pred : Predictions (num_samples,)\n",
        "        \"\"\"\n",
        "        return X @ self.theta + self.bias  # Linear prediction (X * theta + bias)\n",
        "\n",
        "    def mse(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Compute Mean Squared Error (MSE) as a performance metric.\n",
        "\n",
        "        Parameters:\n",
        "        y_true : True target values (ground truth)\n",
        "        y_pred : Predicted values from the model\n",
        "\n",
        "        Returns:\n",
        "        mse : Mean Squared Error, a measure of how close the predictions are to the true values.\n",
        "        \"\"\"\n",
        "        return np.mean((y_true - y_pred) ** 2)  # Calculate the average of the squared differences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kjky5fZ_7t_Z",
        "outputId": "ded25616-d58f-4d74-a85d-a046ab012cb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions: [ 5.79274902  7.77612778  9.03612325 11.01950201]\n",
            "MSE: 0.023689238600062532\n"
          ]
        }
      ],
      "source": [
        "# Sample data\n",
        "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
        "y = np.array([6, 8, 9, 11])\n",
        "\n",
        "# Initialize model\n",
        "model = ElasticNetLinearRegression(alpha=0.1, l1_ratio=0.5, learning_rate=0.01, iterations=1000)\n",
        "\n",
        "# Fit the model to data\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict using the model\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Print predictions and the MSE\n",
        "print(\"Predictions:\", y_pred)\n",
        "print(\"MSE:\", model.mse(y, y_pred))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
